# -*- coding: utf-8 -*-
"""Stock_analysis_with_finBERT6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x9JbDr_5cm6UKTueho_nwhkNxu1c6D-A
"""



# Install necessary libraries
!pip install requests pandas transformers ipywidgets torch plotly sentence-transformers datasets accelerate bitsandbytes

# Enable ipywidgets for Google Colab
from google.colab import output
output.enable_custom_widget_manager()

#API Key KA1DC5SYOT5WUAIL 313d44edfdad48f9bacdfb1d2441ea82

import requests
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, TrainingArguments, Trainer, default_data_collator, EarlyStoppingCallback, BartTokenizer, BartForConditionalGeneration
import torch
import ipywidgets as widgets
from IPython.display import display, HTML
import re
import plotly.graph_objects as go
from sentence_transformers import SentenceTransformer
from datasets import load_dataset, load_metric
from torch.utils.data import DataLoader
from sklearn.metrics import precision_recall_fscore_support

# Fubction to fetch intraday stock data
def get_stock_data(api_key, symbol):
    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval=1min&apikey={api_key}'
    response = requests.get(url)
    data = response.json()

    if 'Time Series (1min)' in data:
        df = pd.DataFrame(data['Time Series (1min)']).T
        df = df.rename(columns={
            '1. open': 'open',
            '2. high': 'high',
            '3. low': 'low',
            '4. close': 'close',
            '5. volume': 'volume'
        })
        df.index = pd.to_datetime(df.index)
        return df
    else:
        raise ValueError(f"Data not available or API request failed. Response: {data}")

# Function to fetch financial news
def get_latest_news(symbol, api_key):
    news_url = f'https://newsapi.org/v2/everything?q={symbol}&apiKey={api_key}'
    response = requests.get(news_url)
    return response.json()

# Clean text function
def clean_text(text):
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\S+@\S+', '', text)  # Remove emails
    text = re.sub(r'[^A-Za-z0-9\s]+', '', text)  # Remove special characters
    text = text.strip()
    return text

# Load models
finbert_tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone')
finbert_model = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')
bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
sentence_encoder = SentenceTransformer('all-mpnet-base-v2')

# Function embeeding
def get_embeddings(text):
    embeddings = sentence_encoder.encode(text)
    return embeddings

# Create Custom dataset class
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class MetricsCalculator:
    def __init__(self):
        self.metric = load_metric("accuracy")
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def compute_metrics(self, eval_pred):
        predictions = eval_pred.predictions
        labels = eval_pred.label_ids

        # Calculate accuracy
        accuracy = self.metric.compute(predictions=predictions.argmax(axis=1), references=labels)

        # Calculate loss
        loss = self.loss_fn(torch.tensor(predictions), torch.tensor(labels)).item()

        # Calculate precision, recall, f1-score
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions.argmax(axis=1), average='weighted')

        return {
            'eval_accuracy': accuracy['accuracy'],
            'eval_loss': loss,
            'eval_precision': precision,
            'eval_recall': recall,
            'eval_f1': f1,
        }

# Function to fine-tune a pre-trained model
def custom_fine_tune(transformer_model, tokenizer, train_dataset, val_dataset, output_dir, epochs=3, batch_size=16, learning_rate=2e-5):
    training_args = TrainingArguments(
        output_dir=output_dir,  # Specify the output directory for saving results
        per_device_train_batch_size=batch_size,
        num_train_epochs=epochs,
        per_device_eval_batch_size=batch_size,
        eval_strategy='epoch',  # Updated according to warning
        save_strategy='epoch',
        logging_dir='./logs',
        fp16=True,
        gradient_accumulation_steps=2,
        learning_rate=learning_rate,
        load_best_model_at_end=True,
        metric_for_best_model='accuracy',
        logging_steps=100,
        weight_decay=0.01,
    )

    num_labels = 2
    model = AutoModelForSequenceClassification.from_pretrained(
        'bert-base-uncased',
        num_labels=num_labels,
        hidden_dropout_prob=0.4,
        attention_probs_dropout_prob=0.4
    )

    # Instantiate your MetricsCalculator
    metrics_calculator = MetricsCalculator()

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=default_data_collator,
        compute_metrics=metrics_calculator.compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
    )

    # Start training
    trainer.train()

    # Evaluate the model
    eval_results = trainer.evaluate()

    # Save the model and tokenizer
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    return eval_results

# Function to generate advice using BART
def generate_advice(prompt):
    inputs = bart_tokenizer.encode("summarize: " + prompt, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = bart_model.generate(inputs, max_length=512, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)
    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Function to summarize text using BART
def summarize_text(text):
    inputs = bart_tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = bart_model.generate(inputs, max_length=300, min_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)
    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Function for stock analysis before fine-tuning
def analyze_stock_before_finetune(alpha_vantage_api_key, news_api_key, symbol):
    stock_data = get_stock_data(alpha_vantage_api_key, symbol)
    latest_price = stock_data['close'][0]

    # Fetch latest news
    news_data = get_latest_news(symbol, news_api_key)
    news_summary_cleaned = []

    if news_data and 'articles' in news_data:
        for article in news_data['articles'][:5]:
            description = clean_text(article['description'])
            inputs = finbert_tokenizer(description, return_tensors='pt', padding=True, truncation=True, max_length=512)
            outputs = finbert_model(**inputs)
            sentiment = torch.argmax(outputs.logits, dim=1).item()
            sentiment_label = "Positive" if sentiment == 1 else "Negative" if sentiment == 0 else "Neutral"

            # Summarize news using finbert model
            summary = summarize_text(description)

            # Get embeddings of the cleaned text
            embeddings = get_embeddings(description)

            news_summary_cleaned.append((article['title'], sentiment_label, summary, embeddings))

    # Investment advice with detailed prompt
    advice_prompt = (
        f"Current market trends show a bullish movement in the tech sector. {symbol} has been gaining momentum. "
        f"News articles suggest significant investor interest. Given the current price of {symbol}, generate detailed investment advice "
        f"considering market trends, stock performance, and financial news. "
        f"Provide a thorough analysis including potential risks, market conditions, and long-term investment potential. "
        f"Discuss the stock's historical performance, recent news impact, and any upcoming events that may influence its price. "
        f"Also, offer a strategic plan for both short-term and long-term investors."
    )

    advice = generate_advice(advice_prompt)

    return latest_price, advice, news_summary_cleaned, stock_data

# Create UI before fine-tuning
alpha_vantage_api_key_widget = widgets.Text(value='', placeholder='Enter your Alpha Vantage API Key', description='Alpha Vantage API Key:')
news_api_key_widget = widgets.Text(value='', placeholder='Enter your News API Key', description='News API Key:')
symbol_widget = widgets.Text(value='', placeholder='Enter stock symbol (e.g., AAPL)', description='Stock Symbol:')
output = widgets.Output()

def on_button_click(b):
    with output:
        output.clear_output()
        alpha_vantage_api_key = alpha_vantage_api_key_widget.value
        news_api_key = news_api_key_widget.value
        symbol = symbol_widget.value
        if alpha_vantage_api_key and news_api_key and symbol:
            latest_price, advice, news_summary_cleaned, stock_data = analyze_stock_before_finetune(alpha_vantage_api_key, news_api_key, symbol)
            display(HTML(f'<h3>Current Price of {symbol}: {latest_price}</h3>'))
            display(HTML(f'<h4>Investment Advice:</h4><p>{advice}</p>'))
            display(HTML('<h4>Latest Financial News:</h4>'))
            for title, sentiment, summary, embeddings in news_summary_cleaned:
                #display(HTML(f"<b>{title}</b> (Sentiment: {sentiment})<br>{summary}<br>Embeddings: {embeddings}<br><br>"))
                display(HTML(f"<b>{title}</b> (Sentiment: {sentiment})<br>{summary}<br><br>"))

            # Visualize stock data
            fig = go.Figure(data=[go.Candlestick(
                x=stock_data.index,
                open=stock_data['open'],
                high=stock_data['high'],
                low=stock_data['low'],
                close=stock_data['close']
            )])
            fig.update_layout(title=f'Stock Price Data for {symbol}', yaxis_title='Price (USD)', xaxis_title='Time')
            fig.show()
        else:
            print("Please enter the Alpha Vantage API key, News API key and stock symbol.")

button = widgets.Button(description="Analyze Stock")
button.on_click(on_button_click)

#API Key KA1DC5SYOT5WUAIL313d44edfdad48f9bacdfb1d2441ea82

# Display UI before fine-tuning
display(alpha_vantage_api_key_widget, news_api_key_widget, symbol_widget, button, output)

# Fine-tuning example (run separately)
# Load datasets
dataset = load_dataset('imdb')

# Split into training and validation sets
train_texts = dataset['train']['text']
train_labels = dataset['train']['label']
val_texts = dataset['test']['text']
val_labels = dataset['test']['label']

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Create custom datasets
train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=128)
val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=128)

# Load the pre-trained model
pretrained_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Evaluate the pre-trained model before fine-tuning

# Instantiate your MetricsCalculator
metrics_calculator = MetricsCalculator()

# Evaluate the pre-trained model before fine-tuning
training_args = TrainingArguments(
    output_dir='./results_before',
    per_device_eval_batch_size=16,
    logging_dir='./logs',
    fp16=True,
    gradient_accumulation_steps=2,
)

trainer_before = Trainer(
    model=pretrained_model,
    args=training_args,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=metrics_calculator.compute_metrics
)

eval_results_before = trainer_before.evaluate()

# Print evaluation results before fine-tuning
print("Evaluation results before fine-tuning:")
print(f"Loss: {eval_results_before['eval_loss']}")
print(f"Accuracy: {eval_results_before['eval_accuracy']}")
print(f"Precision: {eval_results_before['eval_precision']}")
print(f"Recall: {eval_results_before['eval_recall']}")
print(f"F1-score: {eval_results_before['eval_f1']}")

# Fine-tune the pre-trained model with 20 epoch
eval_results_after = custom_fine_tune(pretrained_model, tokenizer, train_dataset, val_dataset, output_dir='./results_after_finetune', epochs=20)

# Print evaluation results after fine-tuning
print("Evaluation results after fine-tuning:")
print(f"Loss: {eval_results_after['eval_loss']}")
print(f"Accuracy: {eval_results_after['eval_accuracy']}")
print(f"Precision: {eval_results_after['eval_precision']}")
print(f"Recall: {eval_results_after['eval_recall']}")
print(f"F1-score: {eval_results_after['eval_f1']}")

# Load the fine-tuning model
fine_tuned_model = AutoModelForSequenceClassification.from_pretrained('./results_after_finetune')
fine_tuned_tokenizer = AutoTokenizer.from_pretrained('./results_after_finetune')

# Save model after fine-tuning
fine_tuned_model.save_pretrained('./saved_model')
fine_tuned_tokenizer.save_pretrained('./saved_model')

# Function for stock analysis after fine-tuning
def analyze_stock_after_finetune(alpha_vantage_api_key, news_api_key, symbol):
    stock_data = get_stock_data(alpha_vantage_api_key, symbol)
    latest_price = stock_data['close'].iloc[0]

    news_data = get_latest_news(symbol, news_api_key)
    news_summary_cleaned = []

    if news_data and 'articles' in news_data:
        for article in news_data['articles'][:5]:
            description = clean_text(article['description'])
            inputs = fine_tuned_tokenizer(description, return_tensors='pt', padding=True, truncation=True, max_length=512)
            outputs = fine_tuned_model(**inputs)
            sentiment = torch.argmax(outputs.logits, dim=1).item()
            sentiment_label = "Positive" if sentiment == 1 else "Negative" if sentiment == 0 else "Neutral"

            news_summary_cleaned.append((article['title'], sentiment_label, description))

    advice_prompt = (
        f"Current market trends show a bullish movement in the tech sector. {symbol} has been gaining momentum. "
        f"News articles suggest significant investor interest. Given the current price of {symbol}, generate detailed investment advice "
        f"considering market trends, stock performance, and financial news. "
        f"Provide a thorough analysis including potential risks, market conditions, and long-term investment potential. "
        f"Discuss the stock's historical performance, recent news impact, and any upcoming events that may influence its price. "
        f"Also, offer a strategic plan for both short-term and long-term investors."
    )

    advice = generate_advice(advice_prompt)
    return latest_price, advice, news_summary_cleaned

# Create UI after fine-tuning
alpha_vantage_api_key_widget = widgets.Text(value='', placeholder='Enter your Alpha Vantage API Key', description='Alpha Vantage API Key:')
news_api_key_widget = widgets.Text(value='', placeholder='Enter your News API Key', description='News API Key:')
symbol_widget = widgets.Text(value='', placeholder='Enter stock symbol (e.g., AAPL)', description='Stock Symbol:')
output = widgets.Output()

def on_button_click(b):
    with output:
        output.clear_output()
        alpha_vantage_api_key = alpha_vantage_api_key_widget.value
        news_api_key = news_api_key_widget.value
        symbol = symbol_widget.value
        if alpha_vantage_api_key and news_api_key and symbol:
            try:
                latest_price, advice, news_summary_cleaned = analyze_stock_after_finetune(alpha_vantage_api_key, news_api_key, symbol)

                display(HTML(f'<h3>Current Price of {symbol}: {latest_price}</h3>'))
                display(HTML(f'<h4>Investment Advice:</h4><p>{advice}</p>'))
                display(HTML('<h4>Latest Financial News:</h4>'))

                for title, sentiment, summary in news_summary_cleaned:
                    display(HTML(f"<b>{title}</b> (Sentiment: {sentiment})<br>{summary}<br><br>"))

                # Visualize stock data separately
                stock_data = get_stock_data(alpha_vantage_api_key, symbol)
                fig = go.Figure(data=[go.Candlestick(
                    x=stock_data.index,
                    open=stock_data['open'],
                    high=stock_data['high'],
                    low=stock_data['low'],
                    close=stock_data['close']
                )])
                fig.update_layout(title=f'Stock Price Data for {symbol}', yaxis_title='Price (USD)', xaxis_title='Time')
                fig.show()
            except Exception as e:
                print(f"An error occurred: {e}")
        else:
            print("Please enter the Alpha Vantage API key, News API key, and stock symbol.")

button = widgets.Button(description="Analyze Stock")
button.on_click(on_button_click)

#API Key KA1DC5SYOT5WUAIL313d44edfdad48f9bacdfb1d2441ea82

# Display UI after fine-tuning
display(alpha_vantage_api_key_widget, news_api_key_widget, symbol_widget, button, output)

# Load saved model
saved_model_path = './saved_model'
fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(saved_model_path)
fine_tuned_tokenizer = AutoTokenizer.from_pretrained(saved_model_path)